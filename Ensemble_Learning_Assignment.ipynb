{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical Questions: Bagging, Random Forest & Ensemble Techniques\n",
        "\n",
        "## Question 1: Can we use Bagging for regression problems?\n",
        "Answer:  \n",
        "Yes, Bagging can be used for regression problems. In regression, Bagging aggregates predictions from multiple base regressors (like Decision Trees) using averaging to reduce variance and improve accuracy.\n",
        "\n",
        "## Question 2: What is the difference between multiple model training and single model training?\n",
        "Answer:  \n",
        "- Single model training: One model is trained on the entire dataset.  \n",
        "- Multiple model training (ensemble): Multiple models are trained on different subsets or variations of the data and combined to improve overall performance and robustness.\n",
        "\n",
        "## Question 3: Explain the concept of feature randomness in Random Forest.\n",
        "Answer:  \n",
        "Random Forest selects a random subset of features at each split of a tree, which introduces feature randomness. This reduces correlation between trees, improving model generalization.\n",
        "\n",
        "## Question 4: What is OOB (Out-of-Bag) Score?\n",
        "Answer:  \n",
        "OOB score is an internal validation score in Bagging/Random Forest. Each tree is trained on a bootstrap sample, leaving out ~1/3 of the data (OOB samples). These are used to estimate model performance without a separate test set.\n",
        "\n",
        "## Question 5: How can you measure the importance of features in a Random Forest model?\n",
        "Answer:  \n",
        "Feature importance can be measured using:  \n",
        "- Mean decrease in impurity (Gini or MSE reduction)  \n",
        "- Permutation importance (impact on model performance when feature values are shuffled)\n",
        "\n",
        "## Question 6: Explain the working principle of a Bagging Classifier.\n",
        "Answer:  \n",
        "- Bagging creates multiple bootstrap samples from the training data.  \n",
        "- A base classifier (e.g., Decision Tree) is trained on each sample.  \n",
        "- Predictions are combined via majority voting (classification) or averaging (regression) to reduce variance.\n",
        "\n",
        "## Question 7: How do you evaluate a Bagging Classifier's performance?\n",
        "Answer:  \n",
        "- Accuracy, Precision, Recall, F1-score (classification)  \n",
        "- OOB score for internal validation  \n",
        "- Confusion matrix for detailed evaluation  \n",
        "\n",
        "## Question 8: How does a Bagging Regressor work?\n",
        "Answer:  \n",
        "- Multiple regressors are trained on different bootstrap samples.  \n",
        "- Predictions from each regressor are averaged to produce the final output, reducing variance.\n",
        "\n",
        "## Question 9: What is the main advantage of ensemble techniques?\n",
        "Answer:  \n",
        "Ensemble techniques reduce variance, bias, or improve predictions by combining multiple models, making them more robust and accurate than single models.\n",
        "\n",
        "## Question 10: What is the main challenge of ensemble methods?\n",
        "Answer:  \n",
        "- Increased computational cost and memory usage  \n",
        "- Harder to interpret compared to single models\n",
        "\n",
        "## Question 11: Explain the key idea behind ensemble techniques.\n",
        "Answer:  \n",
        "The key idea is \"wisdom of the crowd\": combining multiple models leads to better predictions than relying on a single model.\n",
        "\n",
        "## Question 12: What is a Random Forest Classifier?\n",
        "Answer:  \n",
        "A Random Forest Classifier is an ensemble of Decision Trees using Bagging and feature randomness. Trees vote for the most popular class for classification.\n",
        "\n",
        "## Question 13: What are the main types of ensemble techniques?\n",
        "Answer:  \n",
        "1. Bagging (Bootstrap Aggregating)  \n",
        "2. Boosting (e.g., AdaBoost, XGBoost)  \n",
        "3. Stacking  \n",
        "4. Voting ensembles  \n",
        "\n",
        "## Question 14: What is ensemble learning in machine learning?\n",
        "Answer:  \n",
        "Ensemble learning combines predictions from multiple models to improve generalization, reduce errors, and enhance robustness.\n",
        "\n",
        "## Question 15: When should we avoid using ensemble methods?\n",
        "Answer:  \n",
        "- Small datasets (overfitting risk)  \n",
        "- When interpretability is crucial  \n",
        "- Limited computational resources  \n",
        "\n",
        "## Question 16: How does Bagging help in reducing overfitting?\n",
        "Answer:  \n",
        "By training multiple models on bootstrap samples and averaging their predictions, Bagging reduces model variance, thus lowering overfitting.\n",
        "\n",
        "## Question 17: Why is Random Forest better than a single Decision Tree?\n",
        "Answer:  \n",
        "Random Forest is more robust because it reduces overfitting and variance by averaging predictions from multiple trees with feature randomness.\n",
        "\n",
        "## Question 18: What is the role of bootstrap sampling in Bagging?\n",
        "Answer:  \n",
        "Bootstrap sampling creates random subsets of the data with replacement, allowing each base model to see different data and reducing variance.\n",
        "\n",
        "## Question 19: What are some real-world applications of ensemble techniques?\n",
        "Answer:  \n",
        "- Fraud detection  \n",
        "- Customer churn prediction  \n",
        "- Stock price forecasting  \n",
        "- Medical diagnosis  \n",
        "- Credit scoring  \n",
        "\n",
        "## Question 20: What is the difference between Bagging and Boosting?\n",
        "Answer:  \n",
        "| Feature       | Bagging                  | Boosting                   |\n",
        "|---------------|--------------------------|----------------------------|\n",
        "| Approach      | Parallel training        | Sequential training        |\n",
        "| Goal          | Reduce variance          | Reduce bias and variance   |\n",
        "| Weighting     | Equal weights for models | Later models focus on mistakes |\n",
        "| Examples      | Random Forest           | AdaBoost, XGBoost          |\n"
      ],
      "metadata": {
        "id": "UWvRJ7fWQ4Kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical Exercises: Bagging, Random Forest & Ensemble Models\n",
        "\n",
        "# Question 21: Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
        "\"\"\"\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging classifier\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\"\"\"\n",
        "\n",
        "# Question 22: Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
        "\"\"\"\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging Regressor\n",
        "bag_reg = BaggingRegressor(DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "bag_reg.fit(X_train, y_train)\n",
        "y_pred = bag_reg.predict(X_test)\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
        "\"\"\"\n",
        "\n",
        "# Question 23: Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
        "\"\"\"\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X, y)\n",
        "print(\"Feature Importances:\", rf_clf.feature_importances_)\n",
        "\"\"\"\n",
        "\n",
        "# Question 24: Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
        "\"\"\"\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Random Forest\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "print(\"Random Forest MSE:\", mean_squared_error(y_test, y_pred_rf))\n",
        "\n",
        "# Single Decision Tree\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg.fit(X_train, y_train)\n",
        "y_pred_dt = dt_reg.predict(X_test)\n",
        "print(\"Decision Tree MSE:\", mean_squared_error(y_test, y_pred_dt))\n",
        "\"\"\"\n",
        "\n",
        "# Question 25: Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
        "\"\"\"\n",
        "rf_clf_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf_clf_oob.fit(X, y)\n",
        "print(\"OOB Score:\", rf_clf_oob.oob_score_)\n",
        "\"\"\"\n",
        "\n",
        "# Question 26: Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "\"\"\"\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "bag_svm = BaggingClassifier(SVC(), n_estimators=10, random_state=42)\n",
        "bag_svm.fit(X_train, y_train)\n",
        "y_pred = bag_svm.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\"\"\"\n",
        "\n",
        "# Question 27: Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "\"\"\"\n",
        "for n in [10, 50, 100, 200]:\n",
        "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    print(f\"n_estimators={n}, Accuracy={accuracy_score(y_test, y_pred)}\")\n",
        "\"\"\"\n",
        "\n",
        "# Question 28: Train a Bagging Classifier using Logistic Regression as base estimator and print AUC score\n",
        "\"\"\"\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "bag_lr = BaggingClassifier(LogisticRegression(), n_estimators=10, random_state=42)\n",
        "bag_lr.fit(X_train, y_train)\n",
        "y_pred_prob = bag_lr.predict_proba(X_test)[:,1]\n",
        "print(\"AUC Score:\", roc_auc_score(y_test, y_pred_prob))\n",
        "\"\"\"\n",
        "\n",
        "# Question 29: Train a Random Forest Regressor and analyze feature importance scores\n",
        "\"\"\"\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "print(\"Feature Importances:\", rf_reg.feature_importances_)\n",
        "\"\"\"\n",
        "\n",
        "# Question 30: Train an ensemble model using both Bagging and Random Forest and compare accuracy\n",
        "\"\"\"\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred_bag = bag_clf.predict(X_test)\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "\n",
        "print(\"Bagging Accuracy:\", accuracy_score(y_test, y_pred_bag))\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Assignment: Bagging & Random Forest | Questions 31–45\n",
        "# Colab-Ready Template with 20-Mark Explanations\n",
        "# ------------------------------\n",
        "\n",
        "# Import common libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, precision_recall_curve\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, fetch_california_housing, load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ------------------------------\n",
        "# Question 31\n",
        "# ------------------------------\n",
        "# Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "\n",
        "\"\"\"\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "param_grid = {'n_estimators':[50,100,150], 'max_depth':[None,5,10]}\n",
        "grid_search = GridSearchCV(rf_clf, param_grid, cv=3)\n",
        "grid_search.fit(X_train, y_train)\n",
        "y_pred = grid_search.predict(X_test)\n",
        "print(\"Best Params:\", grid_search.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\"\"\"\n",
        "\n",
        "# Answer\n",
        "# GridSearchCV allows tuning hyperparameters like number of trees and depth in Random Forest.\n",
        "# This optimizes accuracy and generalization on unseen data.\n",
        "\n",
        "# ------------------------------\n",
        "# Question 32\n",
        "# ------------------------------\n",
        "# Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
        "\n",
        "\"\"\"\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "for n in [5,10,20]:\n",
        "    bag_reg = BaggingRegressor(DecisionTreeClassifier(), n_estimators=n, random_state=42)\n",
        "    bag_reg.fit(X_train, y_train)\n",
        "    y_pred = bag_reg.predict(X_test)\n",
        "    print(f\"n_estimators={n} MSE:\", mean_squared_error(y_test, y_pred))\n",
        "\"\"\"\n",
        "\n",
        "# Answer\n",
        "# Varying the number of base estimators affects variance reduction and predictive stability.\n",
        "# More estimators generally improve performance but increase computation.\n",
        "\n",
        "# ------------------------------\n",
        "# Question 33\n",
        "# ------------------------------\n",
        "# Train a Random Forest Classifier and analyze misclassified samples\n",
        "\n",
        "\"\"\"\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "misclassified = X_test[y_test != y_pred]\n",
        "print(\"Misclassified samples:\", misclassified)\n",
        "\"\"\"\n",
        "\n",
        "# Answer\n",
        "# Analyzing misclassified samples helps understand model weaknesses and guide feature engineering or hyperparameter tuning.\n",
        "\n",
        "# ------------------------------\n",
        "# Question 34\n",
        "# ------------------------------\n",
        "# Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier\n",
        "\n",
        "\"\"\"\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred_bag = bag_clf.predict(X_test)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
        "print(\"Bagging Accuracy:\", accuracy_score(y_test, y_pred_bag))\n",
        "\"\"\"\n",
        "\n",
        "# Answer\n",
        "# Bagging reduces variance by averaging multiple trees’ predictions. It usually outperforms a single tree in terms of stability and accuracy.\n",
        "\n",
        "# ------------------------------\n",
        "# Question 35\n",
        "# ------------------------------\n",
        "# Train a Random Forest Classifier and visualize the confusion matrix\n",
        "\n",
        "\"\"\"\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\"\"\"\n",
        "\n",
        "# Answer\n",
        "# Confusion matrix shows model performance per class, helping identify which classes are misclassified.\n",
        "\n",
        "# ------------------------------\n",
        "# Question 36\n",
        "# ------------------------------\n",
        "# Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy\n",
        "\n",
        "\"\"\"\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "estimators = [('dt', DecisionTreeClassifier()), ('svc', SVC(probability=True)), ('lr', LogisticRegression())]\n",
        "stack_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "stack_clf.fit(X_train, y_train)\n",
        "y_pred = stack_clf.predict(X_test)\n",
        "print(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\"\"\"\n",
        "\n",
        "# Answer\n",
        "# Stacking combines predictions from multiple models to improve accuracy and leverage different model strengths.\n",
        "\n",
        "# ------------------------------\n",
        "# Question 37\n",
        "# ------------------------------\n",
        "# Train a Random Forest Classifier and print the top 5 most important features\n",
        "\n",
        "\"\"\"\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X, y)\n",
        "importances = rf_clf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1][:5]\n",
        "print(\"Top 5 Features:\", [X.columns[i] if hasattr(X, 'columns') else i for i in indices])\n",
        "\"\"\"\n",
        "\n",
        "# Answer\n",
        "# Feature importance scores highlight which features contribute most to predictions, useful for model interpretation and feature selection.\n",
        "\n",
        "# ------------------------------\n",
        "# Question 38\n",
        "# ------------------------------\n",
        "# Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n",
        "\n",
        "\"\"\"\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred, average='macro'))\n",
        "\"\"\"\n",
        "\n",
        "# Answer\n",
        "# Evaluating precision, recall, and F1 ensures balanced performance across classes, especially when class distribution is uneven.\n",
        "\n",
        "# ------------------------------\n",
        "# Question 39\n",
        "# ------------------------------\n",
        "# Train a Random Forest Classifier and analyze the effect of max depth on accuracy\n",
        "\n",
        "\"\"\"\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "for depth in [None, 2, 3, 4]:\n",
        "    rf_clf = RandomForestClassifier(max_depth=depth, n_estimators=100, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    print(f\"Max Depth={depth} Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\"\"\"\n",
        "\n",
        "# Answer\n",
        "# Controlling max_depth prevents overfitting. Shallow trees reduce variance but may underfit, deep trees may overfit.\n",
        "\n",
        "# ------------------------------\n",
        "# Question 40\n",
        "# ------------------------------\n",
        "# Train a Bagging Regressor using different base estimators (Decision Tree and KNeighbors) and compare performance\n",
        "\n",
        "\"\"\"\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "estimators = [DecisionTreeClassifier(), KNeighborsClassifier()]\n",
        "for est in estimators:\n",
        "    bag_reg = BaggingRegressor(est, n_estimators=10, random_state=42)\n",
        "    bag_reg.fit(X_train, y_train)\n",
        "    y_pred = bag_reg.predict(X_test)\n",
        "    print(f\"{type(est).__name__} MSE:\", mean_squared_error(y_test, y_pred))\n",
        "\"\"\"\n",
        "\n",
        "# Answer\n",
        "# Using different base estimators shows how the choice of model impacts Bagging performance. Decision Trees usually give higher variance reduction.\n",
        "\n",
        "# ------------------------------\n",
        "# Question 41\n",
        "# ------------------------------\n",
        "# Train a Random Forest Classifier and evaluate performance using ROC-AUC Score\n",
        "\n",
        "\"\"\"\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_proba = rf_clf.predict_proba(X_test)[:,1]\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_proba))\n",
        "\"\"\"\n",
        "\n",
        "# Answer\n",
        "# ROC-AUC quantifies model’s ability to distinguish classes; higher values indicate better discriminative power.\n",
        "\n",
        "# ------------------------------\n",
        "# Question 42\n",
        "# ------------------------------\n",
        "# Train a Bagging Classifier and evaluate performance using cross-validation\n",
        "\n",
        "\"\"\"\n",
        "X, y = load_iris(return_X_y=True)\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "scores = cross_val_score(bag_clf, X, y, cv=5)\n",
        "print(\"Cross-Validation Accuracy Scores:\", scores)\n",
        "print(\"Mean CV Accuracy:\", np.mean(scores))\n",
        "\"\"\"\n",
        "\n",
        "# Answer\n",
        "# Cross-validation ensures model stability and generalization across multiple splits of the dataset.\n",
        "\n",
        "# ------------------------------\n",
        "# Question 43\n",
        "# ------------------------------\n",
        "# Train a Random Forest Classifier and plot the Precision-Recall curve\n",
        "\n",
        "\"\"\"\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_proba = rf_clf.predict_proba(X_test)[:,1]\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()\n",
        "\"\"\"\n",
        "\n",
        "# Answer\n",
        "# Precision-Recall curve is useful for evaluating models on imbalanced datasets, emphasizing the trade-off between precision and recall.\n",
        "\n",
        "# ------------------------------\n",
        "# Question 44\n",
        "# ------------------------------\n",
        "# Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy\n",
        "\n",
        "\"\"\"\n",
        "X, y = load_iris(return_X_y=True)\n",
        "estimators = [('rf', RandomForestClassifier(n_estimators=50)), ('lr', LogisticRegression())]\n",
        "stack_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "stack_clf.fit(X, y)\n",
        "y_pred = stack_clf.predict(X)\n",
        "print(\"Stacking Accuracy:\", accuracy_score(y, y_pred))\n",
        "\"\"\"\n",
        "\n",
        "# Answer\n",
        "# Combining Random Forest and Logistic Regression via Stacking leverages strengths of each, improving overall accuracy.\n",
        "\n",
        "# ------------------------------\n",
        "# Question 45\n",
        "# ------------------------------\n",
        "# Train a Bagging Regressor with different levels of bootstrap samples and compare performance\n",
        "\n",
        "\"\"\"\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "for bs in [0.5, 0.7, 1.0]:\n",
        "    bag_reg = BaggingRegressor(DecisionTreeClassifier(), n_estimators=10, bootstrap=True, max_samples=bs, random_state=42)\n",
        "    bag_reg.fit(X_train, y_train)\n",
        "    y_pred = bag_reg.predict(X_test)\n",
        "    print(f\"Bootstrap={bs} MSE:\", mean_squared_error(y_test, y_pred))\n",
        "\"\"\"\n",
        "\n",
        "# Answer\n",
        "# Adjusting bootstrap fraction affects diversity of base estimators; smaller samples increase variance reduction but may lose information.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "VGOVnInpSTWk",
        "outputId": "a94bd8e8-d0a2-43a9-82a3-6a4468cb22fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nX, y = fetch_california_housing(return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\nfor bs in [0.5, 0.7, 1.0]:\\n    bag_reg = BaggingRegressor(DecisionTreeClassifier(), n_estimators=10, bootstrap=True, max_samples=bs, random_state=42)\\n    bag_reg.fit(X_train, y_train)\\n    y_pred = bag_reg.predict(X_test)\\n    print(f\"Bootstrap={bs} MSE:\", mean_squared_error(y_test, y_pred))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}